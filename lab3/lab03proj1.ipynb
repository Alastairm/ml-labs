{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>CITS5508 Lab sheet 3</center>\n",
    "\n",
    "**Name:** Alastair Mory<br>\n",
    "**Student number:** 21120848<br>\n",
    "\n",
    "\n",
    "A voting classifier consisting of SVM, logistic regression and gradient descent classifiers will be used on a classification task involving cellular localisation site of proteins from E. coli bacteria. The data set was obtained [here](https://archive.ics.uci.edu/ml/datasets/ecoli). The data set file is unlabeled, however a data set [description file](https://archive.ics.uci.edu/ml/machine-learning-databases/ecoli/ecoli.names) lists both the attribute and class label information.\n",
    "\n",
    "\n",
    "<br><b>Contents</b><br>\n",
    "[1 E. Coli Classification Data](#1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1 Data Visualisation and Statistics](#1.1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2 Data Preprocessing](#1.2)<br>\n",
    "[2 Classification](#2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.1 Support Vector Machine](#2.1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2 Logistic Regression](#2.2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3 Stochastic Gradient Descent](#2.3)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.4 Ensemble Voting](#2.4)<br>\n",
    "[3 Conclusion](#3)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import ensemble, linear_model, metrics, model_selection, preprocessing, svm\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATA_COLUMNS = ['Sequence Name', 'mcg', 'gvh', 'lip',\n",
    "          'chg', 'aac', 'alm1', 'alm2', 'class']\n",
    "\n",
    "COLUMNS_TO_DROP = ['Sequence Name', 'class']\n",
    "CLASSES_TO_DROP = ['omL', 'imL', 'imS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 E. Coli Classification Data <a name=\"1\">\n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "  **Sequence Name:** Accession number for the SWISS-PROT database (removed)<br>\n",
    "  **mcg:** McGeoch's method for signal sequence recognition. <br>\n",
    "  **gvh:** von Heijne's method for signal sequence recognition. <br>\n",
    "  **lip:** von Heijne's Signal Peptidase II consensus sequence score.\n",
    "       Binary attribute. <br>\n",
    "  **chg:** Presence of charge on N-terminus of predicted lipoproteins.\n",
    "\t   Binary attribute. <br>\n",
    "  **aac:** score of discriminant analysis of the amino acid content of\n",
    "\t   outer membrane and periplasmic proteins. <br>\n",
    "  **alm1:** score of the ALOM membrane spanning region prediction program. <br>\n",
    "  **alm2:** score of ALOM program after excluding putative cleavable signal\n",
    "\t   regions from the sequence. <br>\n",
    "       \n",
    "\n",
    "#### Class Distribution (the class is the localisation site):\n",
    "\n",
    "      cp  (cytoplasm)                                    143\n",
    "      im  (inner membrane without signal sequence)        77               \n",
    "      pp  (perisplasm)                                    52\n",
    "      imU (inner membrane, uncleavable signal sequence)   35\n",
    "      om  (outer membrane)                                20\n",
    "      omL (outer membrane lipoprotein)                     5 (removed)\n",
    "      imL (inner membrane lipoprotein)                     2 (removed)\n",
    "      imS (inner membrane, cleavable signal sequence)      2 (removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in\n",
    "data = pd.read_csv('./ecoli.data', delim_whitespace=True, names=DATA_COLUMNS)\n",
    "\n",
    "# Display example rows\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Visualisation and Statistics <a name=\"1.1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of numerical features\n",
    "%matplotlib inline\n",
    "data.hist(bins=50, figsize=(15,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics for numerical features\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots and statistics we can see a variety of distributions, some relatively normal (aac, gvh & mcg), some bimodal (alm1 & alm2), and some binary attributes (chg & lip). Additionally there is one non-numerical attribute ('Sequence Name') that will not be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Preprocessing <a name=\"1.2\">\n",
    "\n",
    "Remove unused attributes and datapoints, perform train / test split and normalise data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unused_data(data: pd.DataFrame,\n",
    "               drop_classes: List[str]=CLASSES_TO_DROP,\n",
    "               drop_columns: List[str]=COLUMNS_TO_DROP\n",
    "              ) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Drop rows containing data for classes in drop_classes and remove columns\n",
    "    contained in drop_columns.\n",
    "    Returns a tuple containing (x, y) where x is a dataframe of attributes\n",
    "    excluding the class labels and y is a series of class labels.\n",
    "    \"\"\"\n",
    "    drop_indexes = []\n",
    "    for i in range(len(data)):\n",
    "        if data.loc[i, 'class'] in drop_classes:\n",
    "            drop_indexes.append(i)\n",
    "    data = data.drop(index=drop_indexes)\n",
    "    data.reset_index()\n",
    "    \n",
    "    y = data['class']\n",
    "    x = data.drop(columns=drop_columns)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Perform 80/20 train test split\n",
    "train, test = model_selection.train_test_split(data,\n",
    "                                               test_size=0.2,\n",
    "                                               train_size=0.8)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Remove unused classes & columns\n",
    "train_x, train_y = drop_unused_data(train)\n",
    "test_x, test_y = drop_unused_data(test)\n",
    "\n",
    "\n",
    "# Normalise data\n",
    "scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "train_x = scaler.transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Classification <a name=\"2\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(clf: Any,  # pretrained classifier\n",
    "                  test_x: pd.DataFrame, test_y: pd.Series,\n",
    "                  clf_name='Classifier') -> None:\n",
    "    \"\"\"\n",
    "    Run prediction using provided trained classifier and show confusion matrix, accuracy and f1 scores. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Run classifier prediction\n",
    "    pred_y = clf.predict(test_x)\n",
    "    \n",
    "    # Calculate accuracy & F1 score\n",
    "    accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "    f1 = metrics.f1_score(test_y, pred_y, average='weighted')\n",
    "    # Place in dataframe for prettier printing\n",
    "    scores = pd.DataFrame(data=[[accuracy, f1]],\n",
    "                          index=[\"\"],\n",
    "                          columns=['Accuracy', 'F1 Score'])\n",
    "    \n",
    "    # Display confusion matrix and metric scores\n",
    "    metrics.plot_confusion_matrix(clf, test_x, test_y, \n",
    "                              normalize='true',\n",
    "                              cmap=plt.cm.Blues,)\n",
    "    print(f\"{clf_name} Metrics:\\n\")\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Support Vector Machine <a name=\"2.1\"/>\n",
    "\n",
    "The support vector classifier was tested with four different kernels (rbf, linear, sigmoid and poly) and linear was found to have the highest accuracy and f1 score for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(kernel='linear').fit(train_x, train_y)\n",
    "\n",
    "print_metrics(clf_svm, test_x, test_y, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Logistic Regression <a name=\"2.2\">\n",
    "    \n",
    "For this logistic regression classifier, the maximum iterations needed to be increased significantly (from default max_iter=100 to max_iter=1000) in order to allow it to converge on an appriopriate classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = linear_model.LogisticRegressionCV(solver='lbfgs', max_iter=1000).fit(train_x, train_y)\n",
    "\n",
    "print_metrics(clf_lr, test_x, test_y, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Stochastic Gradient Descent <a name=\"2.3\">\n",
    "\n",
    "This stochastic gradient descent classifier achieves an accuracy and f1 score in the 80-90% range, this variation is due to the random nature of training this kind of classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_sgd = linear_model.SGDClassifier(loss='hinge').fit(train_x, train_y)\n",
    "\n",
    "print_metrics(clf_sgd, test_x, test_y, 'SGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Ensemble Voting <a name=\"2.4\"/>\n",
    "\n",
    "The ensemble classifier consists of multiple classifiers that 'vote' on the correct class, ultimately selecting the class that is chosen by a preponderance of its constituent classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifiers = [('svm', clf_svm), ('lr', clf_lr), ('sgd', clf_sgd)]\n",
    "clf_voting = ensemble.VotingClassifier(classifiers).fit(train_x, train_y)\n",
    "\n",
    "print_metrics(clf_voting, test_x, test_y, 'Voting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Conclusion <a name=\"3\"/>\n",
    "\n",
    " In this case the support vector and logistic regression classifiers had very close results (generally classifying only 1-2 datapoints differently), so the ensemble voting classifier performed almost identically to them (as with 2/3 classifiers agreeing in almost every case, the SGD classifier's results were largely irrelevant).\n",
    " \n",
    " It should be noted that 3 constituent classifiers (estimators) is the bare minimum for a useful voting classifier (when using 'hard' voting), as such it would be worth investigating voting classifiers with more estimators (either more different kinds of classifiers, or classifiers of the same type with different hyperparameters) and trying soft voting (where class probabilities are used instead of a majority vote).\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
